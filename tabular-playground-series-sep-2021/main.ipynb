{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Tabular playground series - September 2021\n",
    "\n",
    "https://www.kaggle.com/competitions/tabular-playground-series-sep-2021/overview\n",
    "\n",
    "For this competition, you will predict whether a customer made a claim upon an insurance policy. The ground truth claim is binary valued, but a prediction may be any number from 0.0 to 1.0, representing the probability of a claim. The features in this dataset have been anonymized and may contain missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set(rc = {'figure.figsize':(10, 6)})\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from ml_utils.preprocess.missing import check_missingness\n",
    "from ml_utils.preprocess.pipeline import preprocessing_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = {}\n",
    "\n",
    "train = pd.read_csv(\"Data/train.csv\")\n",
    "train.drop('id', axis=1, inplace=True)\n",
    "test = pd.read_csv(\"Data/test.csv\")\n",
    "\n",
    "print(f\"Train shape {train.shape}\")\n",
    "print(f\"Test shape {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_missingness = check_missingness(train)\n",
    "train_missingness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "118 of the columns have some missing data. All of these have low levels of missingness (c1.5%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.histplot(train_missingness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# For now import with mean\n",
    "def impute_missing(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    col_names = df.columns\n",
    "    df = pd.DataFrame(\n",
    "        SimpleImputer(strategy=\"mean\").fit_transform(df),\n",
    "        columns=col_names\n",
    "    )\n",
    "    return df \n",
    "    \n",
    "pipeline[\"impute\"] = (impute_missing, None)\n",
    "\n",
    "train = impute_missing(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "### Basic EDA\n",
    "\n",
    "\n",
    "The training dataset is quite large with almost 1 million rows. Take a random sample of 10% of the data for analysis, before training on all of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_sample = train.sample(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_col_summary(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    # Min max scale so variances are comparable\n",
    "    df = pd.DataFrame(MinMaxScaler().fit_transform(df), columns=df.columns)\n",
    "\n",
    "    summary_fns = {\n",
    "    \"variance\": np.var,\n",
    "    \"mean\": np.mean,\n",
    "    \"median\": np.median\n",
    "    }\n",
    "\n",
    "    _ = []\n",
    "    for name, fn in summary_fns.items():\n",
    "        _.append(df.apply(fn).to_frame(name=name))\n",
    "\n",
    "    summary_df = pd.concat(_, axis=1)\n",
    "\n",
    "    # Ignore var3 as this has already been analysed/dealt with \n",
    "    summary_df = summary_df[~summary_df.index.isin([\"var3\"])]\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "col_summary = get_col_summary(train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Distribution of column variances\n",
    "sns.histplot(col_summary[\"variance\"], bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Distribution of column means\n",
    "sns.histplot(col_summary[\"mean\"], bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Looks like the data has already been normalised, continue to use all features for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class imbalance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample['claim'].value_counts() / len(train_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Outliers\n",
    "\n",
    "A lot of features aren't gaussian so can't reliably use z-scores for outliers.\n",
    "\n",
    "~~Instead use interquartile range.~~\n",
    "\n",
    "Some of the distributions are veryextremely skewed and a lot of the data are getting flagged as outliers.\n",
    "\n",
    "**Come back to this later, potentially look at whether removing the outliers improves that features predictability?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# x = train_sample['f112']\n",
    "# tol = 1.5\n",
    "\n",
    "# uq = np.percentile(x, 75)\n",
    "# lq = np.percentile(x, 25)\n",
    "# iqr = uq - lq\n",
    "\n",
    "# lower_lim = lq - (iqr * tol)\n",
    "# upper_lim = uq + (iqr * tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = preprocessing_pipeline(test, pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Model\n",
    "\n",
    "The data set is very large. To avoid long train times with comparing a bunch of algorithms I'll be exclusively using catboost.\n",
    "\n",
    "Main reasons:\n",
    " - Avoiding excessive training times using slow sklearn models which gradient boosting will likely outperform\n",
    " - Performance increases are likely to be far greater by tuning a single algorithm and improving the quality of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(\"claim\", axis=1)\n",
    "y = train[\"claim\"]\n",
    "\n",
    "# Initialise baseline catboost model with default parameters\n",
    "cb = CatBoostClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CV = 5\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=N_CV)\n",
    "\n",
    "train_score = []\n",
    "valid_score = []\n",
    "test_set_preds = []\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(kfold.split(X, y)):\n",
    "    print(f\"Running fold: {fold}...\")\n",
    "    X_train, y_train = X.iloc[train_idx], y[train_idx]\n",
    "    X_valid, y_valid = X.iloc[valid_idx], y[valid_idx]\n",
    "    \n",
    "    cb.fit(X_train, y_train)\n",
    "    \n",
    "    y_hat_train = cb.predict_proba(X_train)[:, 1]\n",
    "    y_hat_test = cb.predict_proba(X_valid)[:, 1]\n",
    "    \n",
    "    train_score.append(roc_auc_score(y_train, y_hat_train))\n",
    "    valid_score.append(roc_auc_score(y_valid, y_hat_test))\n",
    "    \n",
    "    # Fold prediction on test set\n",
    "    y_hat = cb.predict_proba(test)[:,1]\n",
    "    test_set_preds.append(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    \"fold\": list(range(N_CV)),\n",
    "    \"train\": train_score,\n",
    "    \"valid\": valid_score\n",
    "})\n",
    "results_df = pd.melt(results_df, id_vars=\"fold\", var_name=\"set\",value_name=\"accuracy\")\n",
    "print(f\"Mean train score {np.mean(train_score)}\")\n",
    "print(f\"Mean test score {np.mean(valid_score)}\")\n",
    "sns.barplot(data=results_df, x=\"fold\", y=\"accuracy\", hue=\"set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final predictions and submission"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b48b5f661da551634919f8b0a859136ee1f6eba60cb5505c3a1b5c971c2b388"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
